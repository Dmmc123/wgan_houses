{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "A-0R8YfGdNwe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AN-Vf6rRcJ7d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurating the model and training parameters"
      ],
      "metadata": {
        "id": "kXyDjeHVdpww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 1000       # number of epochs of training\n",
        "batch_size = 64       # size of the batches\n",
        "lr = 0.00005          # learning rate \n",
        "n_cpu = 8             # number of cpu threads to use during batch generation\n",
        "latent_dim = 100      # dimensionality of the latent space\n",
        "img_size = 64         # size of each image dimension\n",
        "channels = 3          # number of image channels\n",
        "n_critic = 5          # number of training steps for discriminator per iter\n",
        "clip_value = 0.01     # lower and upper clip value for disc. weights\n",
        "sample_interval = 100 # interval betwen image samples\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "metadata": {
        "id": "rj5Wx_h4cObU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "BZAHOWOceMkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1kOP1rMIiaU7nPhTJ4M8EsKIwMcvbz1-N\n",
        "!unzip houses.zip\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "G9IkMMRVeN6-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HousesDatasetGAN(Dataset):\n",
        "    '''our custom version of torch dataset for the pictures of houses'''\n",
        "\n",
        "    def __init__(self, dataset_dir, transforms=None):\n",
        "        '''save the paths to images of houses and transforms if needed'''\n",
        "        self.houses_paths = list(map(str, Path(dataset_dir).rglob('*.jpg')))\n",
        "        self.transforms   = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        '''size of list of paths'''\n",
        "        return len(self.houses_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''retrieve an image from path and return it's tranformed version'''\n",
        "\n",
        "        # open the image in 3 channels \n",
        "        image = Image.open(self.houses_paths[idx]).convert('RGB')\n",
        "\n",
        "        # transform it if needed\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        # return in image and indicator that it is a real one\n",
        "        return image, 1"
      ],
      "metadata": {
        "id": "I6c-fUUrfByK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing the augmentations \n",
        "transforms = T.Compose([\n",
        "    T.Resize((img_size, img_size)),\n",
        "    # T.RandomRotation(degrees=45),       #  <-- maybe it's a bad idea\n",
        "    # T.GaussianBlur(kernel_size=(3, 3)), #  <-- same thing\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# creating a train dataset\n",
        "houses_dataset = HousesDatasetGAN('validated', transforms)"
      ],
      "metadata": {
        "id": "oH2K6m5kfKRN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "m1jx_EdzeEHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "b8h3AbsYLUsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorBig(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorBig, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 512),\n",
        "            *block(512, 2048),\n",
        "            *block(2048, 6144),\n",
        "            nn.Linear(6144, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.shape[0], *img_shape)\n",
        "        return img"
      ],
      "metadata": {
        "id": "dnrnYhdYLWnS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.shape[0], *img_shape)\n",
        "        return img"
      ],
      "metadata": {
        "id": "hGXF_f7zcjgB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "YXpHiyeoLqp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminatorBig(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorBig, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 6144),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(6144, 3072),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(3072, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "zmElyERLLuXT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "W9PDWANNLtHk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "4BkaIeKGeHCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uZwrnP8cMw3",
        "outputId": "a3ff499c-b372-4883-8159-c9a9c7456691"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize generator and discriminator\n",
        "pretrained = True\n",
        "if pretrained:\n",
        "    generator = torch.load('drive/MyDrive/g_big_last.pt')\n",
        "    discriminator = torch.load('drive/MyDrive/d_big_last.pt')\n",
        "else:\n",
        "    generator = GeneratorBig()\n",
        "    discriminator = DiscriminatorBig()    \n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    houses_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=lr)\n",
        "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# Tensor type for simplicity\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# Directory for sampels\n",
        "try:\n",
        "    os.mkdir('images')\n",
        "except FileExistsError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "j3hfeGXud_XL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches_done = 30000\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = generator(z).detach()\n",
        "        # Adversarial loss\n",
        "        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Clip weights of discriminator\n",
        "        for p in discriminator.parameters():\n",
        "            p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "        # Train the generator every n_critic iterations\n",
        "        if i % n_critic == 0:\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Generate a batch of images\n",
        "            gen_imgs = generator(z)\n",
        "            # Adversarial loss\n",
        "            loss_G = -torch.mean(discriminator(gen_imgs))\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        if batches_done % sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "        batches_done += 1\n",
        "\n",
        "    print(\n",
        "        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "        % (epoch, n_epochs, batches_done % len(dataloader), len(dataloader), loss_D.item(), loss_G.item())\n",
        "    )"
      ],
      "metadata": {
        "id": "nNdcPo6seVtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b1a969-916e-4456-d49d-c3e45d27c77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/1000] [Batch 0/16] [D loss: -10909.154297] [G loss: 2547.316406]\n",
            "[Epoch 1/1000] [Batch 0/16] [D loss: -9910.179688] [G loss: 5329.849121]\n",
            "[Epoch 2/1000] [Batch 0/16] [D loss: -10330.520508] [G loss: 4062.346436]\n",
            "[Epoch 3/1000] [Batch 0/16] [D loss: -11406.085938] [G loss: -421.159393]\n",
            "[Epoch 4/1000] [Batch 0/16] [D loss: -10672.092773] [G loss: 6739.904785]\n",
            "[Epoch 5/1000] [Batch 0/16] [D loss: -10816.071289] [G loss: -43.364407]\n",
            "[Epoch 6/1000] [Batch 0/16] [D loss: -12087.554688] [G loss: 4306.656738]\n",
            "[Epoch 7/1000] [Batch 0/16] [D loss: -11218.928711] [G loss: 2177.204834]\n",
            "[Epoch 8/1000] [Batch 0/16] [D loss: -10398.196289] [G loss: 3785.508545]\n",
            "[Epoch 9/1000] [Batch 0/16] [D loss: -10905.670898] [G loss: 985.994141]\n",
            "[Epoch 10/1000] [Batch 0/16] [D loss: -11403.240234] [G loss: -5673.678223]\n",
            "[Epoch 11/1000] [Batch 0/16] [D loss: -12188.446289] [G loss: 1856.216187]\n",
            "[Epoch 12/1000] [Batch 0/16] [D loss: -11801.355469] [G loss: -2030.354858]\n",
            "[Epoch 13/1000] [Batch 0/16] [D loss: -11268.133789] [G loss: -2245.888672]\n",
            "[Epoch 14/1000] [Batch 0/16] [D loss: -12023.334961] [G loss: 5874.475098]\n",
            "[Epoch 15/1000] [Batch 0/16] [D loss: -11248.709961] [G loss: 4992.696777]\n",
            "[Epoch 16/1000] [Batch 0/16] [D loss: -10690.744141] [G loss: 1488.499268]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(discriminator, 'drive/MyDrive/d_big_last.pt')\n",
        "torch.save(generator, 'drive/MyDrive/g_big_last.pt')"
      ],
      "metadata": {
        "id": "_B4IUuHrbFD6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "gen_imgs = generator(z)\n",
        "save_image(gen_imgs.data[-25:], \"images/lastN.png\", nrow=5, normalize=True)"
      ],
      "metadata": {
        "id": "oUXYo3Fv3t8U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "* https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan/wgan.py"
      ],
      "metadata": {
        "id": "qHlRiPGl6cD3"
      }
    }
  ]
}