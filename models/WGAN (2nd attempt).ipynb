{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN (2nd attempt).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "pK7KUb_aEE8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "9ZDscr2PDttD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training parameters"
      ],
      "metadata": {
        "id": "YqJrKikLEGs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# makind directory for image samples while training\n",
        "os.makedirs(\"images2\", exist_ok=True)\n",
        "\n",
        "n_epochs = 1000                            # number of epochs of training\n",
        "batch_size = 64                            # size of the batches\n",
        "lr = 0.0002                                # adam: learning rate\n",
        "b1 = 0.5                                   # adam: decay of first order momentum of gradient\n",
        "b2 = 0.999                                 # adam: decay of first order momentum of gradient\n",
        "n_cpu = 8                                  # number of cpu threads to use during batch generation\n",
        "latent_dim = 200                           # dimensionality of the latent space\n",
        "img_size = 64                              # size of each image dimension\n",
        "channels = 3                               # number of image channels\n",
        "n_critic = 5                               # number of training steps for discriminator per iter\n",
        "lip_value = 0.01                           # lower and upper clip value for disc. weights\n",
        "sample_interval = 100                      # interval betwen image samples\n",
        "img_shape = (channels, img_size, img_size) # shape of training sample\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "metadata": {
        "id": "7CJWvE0_ENJY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "EE6026fLEQF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kKTEzxZFy1U",
        "outputId": "ff670971-0691-40d1-e6ca-af4b7f6c9d25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/resized.zip -d resized2\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "Qvgl4FP_FxBh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HousesDatasetGAN(Dataset):\n",
        "    '''our custom version of torch dataset for the pictures of houses'''\n",
        "\n",
        "    def __init__(self, dataset_dir, transforms=None):\n",
        "        '''save the paths to images of houses and transforms if needed'''\n",
        "        self.houses_paths = list(map(str, Path(dataset_dir).rglob('*.jpg')))\n",
        "        self.transforms   = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        '''size of list of paths'''\n",
        "        return len(self.houses_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''retrieve an image from path and return it's tranformed version'''\n",
        "\n",
        "        # open the image in 3 channels \n",
        "        image = Image.open(self.houses_paths[idx]).convert('RGB')\n",
        "\n",
        "        # transform it if needed\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        # return in image and indicator that it is a real one\n",
        "        return image, 1"
      ],
      "metadata": {
        "id": "A6J_l42RGKiV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing the augmentations \n",
        "transforms = T.Compose([\n",
        "    # T.RandomRotation(degrees=45),       #  <-- maybe it's a bad idea\n",
        "    # T.GaussianBlur(kernel_size=(3, 3)), #  <-- same thing\n",
        "    T.RandomHorizontalFlip(),\n",
        "    # T.ColorJitter(brightness=0.4, hue=0.3),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# creating a train dataset\n",
        "houses_dataset = HousesDatasetGAN('resized2', transforms)\n",
        "\n",
        "# Configure data loader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    houses_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "print(len(houses_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebrkmSjyDrvP",
        "outputId": "cd6f7799-e2bb-471b-ab07-97cb4afc1bb8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "1QBDQgPCESQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "kZYJrT9TETKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lnDGiEMZDdf1"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 256, normalize=False),\n",
        "            *block(256, 1024),\n",
        "            *block(1024, 4096),\n",
        "            *block(4096, 16384),\n",
        "            nn.Linear(16384, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.shape[0], *img_shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "K3w9hmrcEVLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 16384),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(16384, 4096),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(4096, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.shape[0], -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "saNT66VREWuY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-SWVrbBdEY4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ],
      "metadata": {
        "id": "iVKiHF3sDmqM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss weight for gradient penalty\n",
        "lambda_gp = 10\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "metadata": {
        "id": "6CbytVVxEZxv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches_done = 0\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = generator(z)\n",
        "\n",
        "        # Real images\n",
        "        real_validity = discriminator(real_imgs)\n",
        "        # Fake images\n",
        "        fake_validity = discriminator(fake_imgs)\n",
        "        # Gradient penalty\n",
        "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
        "        # Adversarial loss\n",
        "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Train the generator every n_critic steps\n",
        "        if i % n_critic == 0:\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            # Generate a batch of images\n",
        "            fake_imgs = generator(z)\n",
        "            # Loss measures generator's ability to fool the discriminator\n",
        "            # Train on fake images\n",
        "            fake_validity = discriminator(fake_imgs)\n",
        "            g_loss = -torch.mean(fake_validity)\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            if batches_done % sample_interval == 0:\n",
        "                save_image(fake_imgs.data[:25], \"images2/%d.png\" % batches_done, nrow=5, normalize=True)\n",
        "\n",
        "            batches_done += n_critic\n",
        "\n",
        "    print(\n",
        "        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "        % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HnN7F-b5Dkgk",
        "outputId": "9a095400-49d7-43fd-f2a0-39e0b601d944"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0/1000] [Batch 101/102] [D loss: -13.722965] [G loss: 11.760313]\n",
            "[Epoch 1/1000] [Batch 101/102] [D loss: -19.565790] [G loss: 6.918622]\n",
            "[Epoch 2/1000] [Batch 101/102] [D loss: -20.401299] [G loss: 6.548736]\n",
            "[Epoch 3/1000] [Batch 101/102] [D loss: -14.634324] [G loss: 6.032339]\n",
            "[Epoch 4/1000] [Batch 101/102] [D loss: -19.619642] [G loss: 17.022682]\n",
            "[Epoch 5/1000] [Batch 101/102] [D loss: -18.874287] [G loss: 8.418646]\n",
            "[Epoch 6/1000] [Batch 101/102] [D loss: -17.798012] [G loss: 21.639175]\n",
            "[Epoch 7/1000] [Batch 101/102] [D loss: -10.534840] [G loss: 3.523641]\n",
            "[Epoch 8/1000] [Batch 101/102] [D loss: -13.618046] [G loss: 12.221556]\n",
            "[Epoch 9/1000] [Batch 101/102] [D loss: -10.235277] [G loss: 8.307076]\n",
            "[Epoch 10/1000] [Batch 101/102] [D loss: -15.042346] [G loss: 11.215281]\n",
            "[Epoch 11/1000] [Batch 101/102] [D loss: -6.928462] [G loss: -4.808434]\n",
            "[Epoch 12/1000] [Batch 101/102] [D loss: -10.057541] [G loss: 3.400860]\n",
            "[Epoch 13/1000] [Batch 101/102] [D loss: -11.281293] [G loss: 12.506516]\n",
            "[Epoch 14/1000] [Batch 101/102] [D loss: -8.921585] [G loss: -1.428704]\n",
            "[Epoch 15/1000] [Batch 101/102] [D loss: -12.032896] [G loss: 21.082449]\n",
            "[Epoch 16/1000] [Batch 101/102] [D loss: -9.009657] [G loss: -0.187484]\n",
            "[Epoch 17/1000] [Batch 101/102] [D loss: -8.923828] [G loss: 4.951923]\n",
            "[Epoch 18/1000] [Batch 101/102] [D loss: -10.075733] [G loss: 16.800316]\n",
            "[Epoch 19/1000] [Batch 101/102] [D loss: -13.210711] [G loss: 12.126938]\n",
            "[Epoch 20/1000] [Batch 101/102] [D loss: -0.922832] [G loss: 6.097306]\n",
            "[Epoch 21/1000] [Batch 101/102] [D loss: -9.777800] [G loss: 3.016149]\n",
            "[Epoch 22/1000] [Batch 101/102] [D loss: -12.046238] [G loss: 1.694958]\n",
            "[Epoch 23/1000] [Batch 101/102] [D loss: -10.250667] [G loss: 16.783943]\n",
            "[Epoch 24/1000] [Batch 101/102] [D loss: -12.461596] [G loss: 3.266558]\n",
            "[Epoch 25/1000] [Batch 101/102] [D loss: -10.405060] [G loss: 5.610252]\n",
            "[Epoch 26/1000] [Batch 101/102] [D loss: -10.836269] [G loss: -3.822506]\n",
            "[Epoch 27/1000] [Batch 101/102] [D loss: -12.473312] [G loss: -3.792595]\n",
            "[Epoch 28/1000] [Batch 101/102] [D loss: -11.931386] [G loss: 3.697330]\n",
            "[Epoch 29/1000] [Batch 101/102] [D loss: -10.203053] [G loss: 12.785242]\n",
            "[Epoch 30/1000] [Batch 101/102] [D loss: -10.955475] [G loss: 13.903267]\n",
            "[Epoch 31/1000] [Batch 101/102] [D loss: -10.331858] [G loss: 6.160359]\n",
            "[Epoch 32/1000] [Batch 101/102] [D loss: -9.477313] [G loss: 2.975239]\n",
            "[Epoch 33/1000] [Batch 101/102] [D loss: -11.519373] [G loss: 5.169721]\n",
            "[Epoch 34/1000] [Batch 101/102] [D loss: -12.535080] [G loss: 4.442358]\n",
            "[Epoch 35/1000] [Batch 101/102] [D loss: -6.571888] [G loss: 9.554063]\n",
            "[Epoch 36/1000] [Batch 101/102] [D loss: -7.007048] [G loss: 10.514073]\n",
            "[Epoch 37/1000] [Batch 101/102] [D loss: -8.661481] [G loss: 4.676103]\n",
            "[Epoch 38/1000] [Batch 101/102] [D loss: -8.143334] [G loss: -2.293070]\n",
            "[Epoch 39/1000] [Batch 101/102] [D loss: -10.250690] [G loss: 0.263746]\n",
            "[Epoch 40/1000] [Batch 101/102] [D loss: -8.897144] [G loss: 13.794552]\n",
            "[Epoch 41/1000] [Batch 101/102] [D loss: -8.345324] [G loss: 10.118618]\n",
            "[Epoch 42/1000] [Batch 101/102] [D loss: -11.637592] [G loss: -8.248059]\n",
            "[Epoch 43/1000] [Batch 101/102] [D loss: -8.743280] [G loss: 7.247416]\n",
            "[Epoch 44/1000] [Batch 101/102] [D loss: -8.204081] [G loss: 5.827843]\n",
            "[Epoch 45/1000] [Batch 101/102] [D loss: -8.996863] [G loss: 11.141043]\n",
            "[Epoch 46/1000] [Batch 101/102] [D loss: -10.648526] [G loss: 9.158319]\n",
            "[Epoch 47/1000] [Batch 101/102] [D loss: -13.272234] [G loss: -1.538386]\n",
            "[Epoch 48/1000] [Batch 101/102] [D loss: -9.589475] [G loss: -7.275385]\n",
            "[Epoch 49/1000] [Batch 101/102] [D loss: -9.932543] [G loss: 12.629681]\n",
            "[Epoch 50/1000] [Batch 101/102] [D loss: -7.784471] [G loss: 2.721766]\n",
            "[Epoch 51/1000] [Batch 101/102] [D loss: -11.736914] [G loss: 6.127354]\n",
            "[Epoch 52/1000] [Batch 101/102] [D loss: -9.622841] [G loss: 7.940943]\n",
            "[Epoch 53/1000] [Batch 101/102] [D loss: -11.375920] [G loss: 6.090958]\n",
            "[Epoch 54/1000] [Batch 101/102] [D loss: -11.481686] [G loss: 10.672112]\n",
            "[Epoch 55/1000] [Batch 101/102] [D loss: -17.002760] [G loss: 3.292643]\n",
            "[Epoch 56/1000] [Batch 101/102] [D loss: -11.181360] [G loss: -9.573406]\n",
            "[Epoch 57/1000] [Batch 101/102] [D loss: -8.080398] [G loss: 9.782468]\n",
            "[Epoch 58/1000] [Batch 101/102] [D loss: -7.969364] [G loss: 3.991712]\n",
            "[Epoch 59/1000] [Batch 101/102] [D loss: -8.608579] [G loss: 1.233452]\n",
            "[Epoch 60/1000] [Batch 101/102] [D loss: -13.517137] [G loss: -6.387005]\n",
            "[Epoch 61/1000] [Batch 101/102] [D loss: -9.427099] [G loss: 1.177275]\n",
            "[Epoch 62/1000] [Batch 101/102] [D loss: -9.102089] [G loss: 8.742901]\n",
            "[Epoch 63/1000] [Batch 101/102] [D loss: -12.009453] [G loss: 6.112482]\n",
            "[Epoch 64/1000] [Batch 101/102] [D loss: -6.251956] [G loss: 0.286886]\n",
            "[Epoch 65/1000] [Batch 101/102] [D loss: -7.294515] [G loss: 3.061038]\n",
            "[Epoch 66/1000] [Batch 101/102] [D loss: -10.117518] [G loss: -3.546786]\n",
            "[Epoch 67/1000] [Batch 101/102] [D loss: -11.437516] [G loss: -9.074703]\n",
            "[Epoch 68/1000] [Batch 101/102] [D loss: -11.456046] [G loss: -4.238036]\n",
            "[Epoch 69/1000] [Batch 101/102] [D loss: -10.117159] [G loss: -6.510004]\n",
            "[Epoch 70/1000] [Batch 101/102] [D loss: -5.740598] [G loss: 12.106638]\n",
            "[Epoch 71/1000] [Batch 101/102] [D loss: -7.773168] [G loss: 12.948195]\n",
            "[Epoch 72/1000] [Batch 101/102] [D loss: -7.392557] [G loss: -4.159563]\n",
            "[Epoch 73/1000] [Batch 101/102] [D loss: -9.544367] [G loss: 2.188631]\n",
            "[Epoch 74/1000] [Batch 101/102] [D loss: -9.344061] [G loss: -4.883329]\n",
            "[Epoch 75/1000] [Batch 101/102] [D loss: -11.383750] [G loss: 3.409787]\n",
            "[Epoch 76/1000] [Batch 101/102] [D loss: -11.391474] [G loss: -4.412356]\n",
            "[Epoch 77/1000] [Batch 101/102] [D loss: -8.563867] [G loss: 5.162923]\n",
            "[Epoch 78/1000] [Batch 101/102] [D loss: -9.529470] [G loss: 7.962766]\n",
            "[Epoch 79/1000] [Batch 101/102] [D loss: -8.371553] [G loss: 0.391930]\n",
            "[Epoch 80/1000] [Batch 101/102] [D loss: -9.331092] [G loss: -0.882336]\n",
            "[Epoch 81/1000] [Batch 101/102] [D loss: -4.554070] [G loss: -3.104367]\n",
            "[Epoch 82/1000] [Batch 101/102] [D loss: -8.853388] [G loss: -1.283797]\n",
            "[Epoch 83/1000] [Batch 101/102] [D loss: -10.502666] [G loss: 5.982641]\n",
            "[Epoch 84/1000] [Batch 101/102] [D loss: -8.725843] [G loss: 10.469170]\n",
            "[Epoch 85/1000] [Batch 101/102] [D loss: -9.162170] [G loss: 13.847860]\n",
            "[Epoch 86/1000] [Batch 101/102] [D loss: -9.459801] [G loss: 15.252110]\n",
            "[Epoch 87/1000] [Batch 101/102] [D loss: -9.016026] [G loss: 0.211607]\n",
            "[Epoch 88/1000] [Batch 101/102] [D loss: -10.787134] [G loss: 7.689103]\n",
            "[Epoch 89/1000] [Batch 101/102] [D loss: -8.915136] [G loss: 4.849359]\n",
            "[Epoch 90/1000] [Batch 101/102] [D loss: -8.439491] [G loss: -0.121193]\n",
            "[Epoch 91/1000] [Batch 101/102] [D loss: -10.081561] [G loss: 12.412567]\n",
            "[Epoch 92/1000] [Batch 101/102] [D loss: -10.656942] [G loss: 12.734516]\n",
            "[Epoch 93/1000] [Batch 101/102] [D loss: -9.290781] [G loss: 16.657438]\n",
            "[Epoch 94/1000] [Batch 101/102] [D loss: -9.074039] [G loss: -3.269152]\n",
            "[Epoch 95/1000] [Batch 101/102] [D loss: -9.556168] [G loss: 7.533553]\n",
            "[Epoch 96/1000] [Batch 101/102] [D loss: -9.785223] [G loss: 0.308344]\n",
            "[Epoch 97/1000] [Batch 101/102] [D loss: -8.082483] [G loss: 0.433252]\n",
            "[Epoch 98/1000] [Batch 101/102] [D loss: -9.179043] [G loss: 4.749911]\n",
            "[Epoch 99/1000] [Batch 101/102] [D loss: -11.387121] [G loss: -2.111882]\n",
            "[Epoch 100/1000] [Batch 101/102] [D loss: -9.515652] [G loss: 7.541313]\n",
            "[Epoch 101/1000] [Batch 101/102] [D loss: -6.875661] [G loss: -0.764299]\n",
            "[Epoch 102/1000] [Batch 101/102] [D loss: -11.079986] [G loss: -5.016869]\n",
            "[Epoch 103/1000] [Batch 101/102] [D loss: -11.538933] [G loss: 6.321854]\n",
            "[Epoch 104/1000] [Batch 101/102] [D loss: -10.933642] [G loss: -0.074865]\n",
            "[Epoch 105/1000] [Batch 101/102] [D loss: -3.527075] [G loss: -0.775905]\n",
            "[Epoch 106/1000] [Batch 101/102] [D loss: -9.696298] [G loss: -7.750470]\n",
            "[Epoch 107/1000] [Batch 101/102] [D loss: -6.642619] [G loss: -0.516533]\n",
            "[Epoch 108/1000] [Batch 101/102] [D loss: -10.557140] [G loss: 2.815358]\n",
            "[Epoch 109/1000] [Batch 101/102] [D loss: -8.980176] [G loss: 11.385723]\n",
            "[Epoch 110/1000] [Batch 101/102] [D loss: -11.310773] [G loss: 0.227051]\n",
            "[Epoch 111/1000] [Batch 101/102] [D loss: -10.198584] [G loss: -3.971923]\n",
            "[Epoch 112/1000] [Batch 101/102] [D loss: -8.022943] [G loss: 15.855993]\n",
            "[Epoch 113/1000] [Batch 101/102] [D loss: -10.051863] [G loss: 3.710198]\n",
            "[Epoch 114/1000] [Batch 101/102] [D loss: -8.620691] [G loss: 2.948015]\n",
            "[Epoch 115/1000] [Batch 101/102] [D loss: -8.922798] [G loss: -3.569106]\n",
            "[Epoch 116/1000] [Batch 101/102] [D loss: -9.252331] [G loss: 3.595930]\n",
            "[Epoch 117/1000] [Batch 101/102] [D loss: -5.206419] [G loss: 0.887870]\n",
            "[Epoch 118/1000] [Batch 101/102] [D loss: -9.309257] [G loss: 1.434176]\n",
            "[Epoch 119/1000] [Batch 101/102] [D loss: -9.300235] [G loss: 6.322014]\n",
            "[Epoch 120/1000] [Batch 101/102] [D loss: -8.996356] [G loss: 5.702163]\n",
            "[Epoch 121/1000] [Batch 101/102] [D loss: -11.687957] [G loss: -0.132711]\n",
            "[Epoch 122/1000] [Batch 101/102] [D loss: -8.496952] [G loss: 4.741281]\n",
            "[Epoch 123/1000] [Batch 101/102] [D loss: -7.806893] [G loss: 9.529009]\n",
            "[Epoch 124/1000] [Batch 101/102] [D loss: -8.846270] [G loss: 1.393744]\n",
            "[Epoch 125/1000] [Batch 101/102] [D loss: -7.655297] [G loss: 4.931122]\n",
            "[Epoch 126/1000] [Batch 101/102] [D loss: -9.089184] [G loss: 26.642155]\n",
            "[Epoch 127/1000] [Batch 101/102] [D loss: -9.982775] [G loss: 6.141260]\n",
            "[Epoch 128/1000] [Batch 101/102] [D loss: -8.259933] [G loss: 9.844385]\n",
            "[Epoch 129/1000] [Batch 101/102] [D loss: -10.082315] [G loss: 2.769489]\n",
            "[Epoch 130/1000] [Batch 101/102] [D loss: -9.429958] [G loss: 3.617544]\n",
            "[Epoch 131/1000] [Batch 101/102] [D loss: -8.660587] [G loss: 4.803031]\n",
            "[Epoch 132/1000] [Batch 101/102] [D loss: -10.355705] [G loss: -3.979546]\n",
            "[Epoch 133/1000] [Batch 101/102] [D loss: -5.702972] [G loss: 5.118392]\n",
            "[Epoch 134/1000] [Batch 101/102] [D loss: -9.221092] [G loss: -4.354926]\n",
            "[Epoch 135/1000] [Batch 101/102] [D loss: -10.332090] [G loss: 2.779254]\n",
            "[Epoch 136/1000] [Batch 101/102] [D loss: -8.770708] [G loss: -4.901732]\n",
            "[Epoch 137/1000] [Batch 101/102] [D loss: -9.281952] [G loss: 3.549888]\n",
            "[Epoch 138/1000] [Batch 101/102] [D loss: -9.326740] [G loss: 0.419939]\n",
            "[Epoch 139/1000] [Batch 101/102] [D loss: -9.067391] [G loss: 8.513565]\n",
            "[Epoch 140/1000] [Batch 101/102] [D loss: -10.672150] [G loss: 5.083508]\n",
            "[Epoch 141/1000] [Batch 101/102] [D loss: -8.859123] [G loss: -0.705529]\n",
            "[Epoch 142/1000] [Batch 101/102] [D loss: -9.513781] [G loss: 5.652412]\n",
            "[Epoch 143/1000] [Batch 101/102] [D loss: -6.823269] [G loss: 1.751381]\n",
            "[Epoch 144/1000] [Batch 101/102] [D loss: -8.626585] [G loss: 1.229434]\n",
            "[Epoch 145/1000] [Batch 101/102] [D loss: -6.372997] [G loss: 22.178410]\n",
            "[Epoch 146/1000] [Batch 101/102] [D loss: -10.962305] [G loss: 3.494642]\n",
            "[Epoch 147/1000] [Batch 101/102] [D loss: -9.279579] [G loss: -2.028104]\n",
            "[Epoch 148/1000] [Batch 101/102] [D loss: -6.838005] [G loss: 1.856066]\n",
            "[Epoch 149/1000] [Batch 101/102] [D loss: -7.963087] [G loss: -0.707180]\n",
            "[Epoch 150/1000] [Batch 101/102] [D loss: -9.156401] [G loss: 6.006020]\n",
            "[Epoch 151/1000] [Batch 101/102] [D loss: -10.218675] [G loss: -2.610751]\n",
            "[Epoch 152/1000] [Batch 101/102] [D loss: -8.904000] [G loss: 11.015556]\n",
            "[Epoch 153/1000] [Batch 101/102] [D loss: -10.220115] [G loss: 0.003005]\n",
            "[Epoch 154/1000] [Batch 101/102] [D loss: -8.544178] [G loss: 9.263062]\n",
            "[Epoch 155/1000] [Batch 101/102] [D loss: -7.387753] [G loss: -1.250377]\n",
            "[Epoch 156/1000] [Batch 101/102] [D loss: -9.002054] [G loss: 2.448922]\n",
            "[Epoch 157/1000] [Batch 101/102] [D loss: -10.434666] [G loss: -4.308661]\n",
            "[Epoch 158/1000] [Batch 101/102] [D loss: -8.205191] [G loss: 5.074706]\n",
            "[Epoch 159/1000] [Batch 101/102] [D loss: -9.869081] [G loss: 1.136293]\n",
            "[Epoch 160/1000] [Batch 101/102] [D loss: -4.308204] [G loss: 18.372564]\n",
            "[Epoch 161/1000] [Batch 101/102] [D loss: -8.233376] [G loss: 3.252976]\n",
            "[Epoch 162/1000] [Batch 101/102] [D loss: -8.900443] [G loss: 14.000105]\n",
            "[Epoch 163/1000] [Batch 101/102] [D loss: -9.378784] [G loss: 14.858428]\n",
            "[Epoch 164/1000] [Batch 101/102] [D loss: -7.623081] [G loss: 13.353897]\n",
            "[Epoch 165/1000] [Batch 101/102] [D loss: -6.370883] [G loss: 14.573142]\n",
            "[Epoch 166/1000] [Batch 101/102] [D loss: -9.390156] [G loss: -2.659584]\n",
            "[Epoch 167/1000] [Batch 101/102] [D loss: -9.675961] [G loss: 7.227441]\n",
            "[Epoch 168/1000] [Batch 101/102] [D loss: -7.342301] [G loss: 9.416453]\n",
            "[Epoch 169/1000] [Batch 101/102] [D loss: -8.639587] [G loss: 16.589569]\n",
            "[Epoch 170/1000] [Batch 101/102] [D loss: -7.984329] [G loss: 0.894972]\n",
            "[Epoch 171/1000] [Batch 101/102] [D loss: -6.473542] [G loss: 15.670372]\n",
            "[Epoch 172/1000] [Batch 101/102] [D loss: -5.610210] [G loss: 4.254662]\n",
            "[Epoch 173/1000] [Batch 101/102] [D loss: -6.625762] [G loss: 4.727892]\n",
            "[Epoch 174/1000] [Batch 101/102] [D loss: -13.980737] [G loss: 1.202078]\n",
            "[Epoch 175/1000] [Batch 101/102] [D loss: -6.250871] [G loss: -1.198167]\n",
            "[Epoch 176/1000] [Batch 101/102] [D loss: -9.790550] [G loss: -15.015068]\n",
            "[Epoch 177/1000] [Batch 101/102] [D loss: -10.963797] [G loss: 18.329716]\n",
            "[Epoch 178/1000] [Batch 101/102] [D loss: -8.184722] [G loss: 8.747931]\n",
            "[Epoch 179/1000] [Batch 101/102] [D loss: -5.967021] [G loss: 5.209698]\n",
            "[Epoch 180/1000] [Batch 101/102] [D loss: -8.838471] [G loss: -0.668717]\n",
            "[Epoch 181/1000] [Batch 101/102] [D loss: -9.443024] [G loss: -2.377236]\n",
            "[Epoch 182/1000] [Batch 101/102] [D loss: -8.056805] [G loss: -0.549912]\n",
            "[Epoch 183/1000] [Batch 101/102] [D loss: -8.193168] [G loss: 2.580823]\n",
            "[Epoch 184/1000] [Batch 101/102] [D loss: -7.725320] [G loss: 11.201204]\n",
            "[Epoch 185/1000] [Batch 101/102] [D loss: -11.532483] [G loss: -3.310413]\n",
            "[Epoch 186/1000] [Batch 101/102] [D loss: -9.526093] [G loss: 0.070210]\n",
            "[Epoch 187/1000] [Batch 101/102] [D loss: -7.412088] [G loss: 18.631119]\n",
            "[Epoch 188/1000] [Batch 101/102] [D loss: -8.532635] [G loss: 4.565748]\n",
            "[Epoch 189/1000] [Batch 101/102] [D loss: -9.706819] [G loss: -4.786042]\n",
            "[Epoch 190/1000] [Batch 101/102] [D loss: -9.698785] [G loss: 3.505956]\n",
            "[Epoch 191/1000] [Batch 101/102] [D loss: -11.072031] [G loss: 0.690190]\n",
            "[Epoch 192/1000] [Batch 101/102] [D loss: -8.696126] [G loss: 15.474781]\n",
            "[Epoch 193/1000] [Batch 101/102] [D loss: -10.456379] [G loss: 1.601496]\n",
            "[Epoch 194/1000] [Batch 101/102] [D loss: -6.085469] [G loss: 3.702431]\n",
            "[Epoch 195/1000] [Batch 101/102] [D loss: -8.785860] [G loss: 6.206259]\n",
            "[Epoch 196/1000] [Batch 101/102] [D loss: -9.012909] [G loss: 6.574238]\n",
            "[Epoch 197/1000] [Batch 101/102] [D loss: -7.733870] [G loss: 9.058346]\n",
            "[Epoch 198/1000] [Batch 101/102] [D loss: -3.853266] [G loss: 10.771946]\n",
            "[Epoch 199/1000] [Batch 101/102] [D loss: -8.515210] [G loss: 5.373736]\n",
            "[Epoch 200/1000] [Batch 101/102] [D loss: -6.393497] [G loss: 1.813766]\n",
            "[Epoch 201/1000] [Batch 101/102] [D loss: -9.299755] [G loss: 4.731634]\n",
            "[Epoch 202/1000] [Batch 101/102] [D loss: -7.588730] [G loss: -9.392071]\n",
            "[Epoch 203/1000] [Batch 101/102] [D loss: -9.665184] [G loss: 16.072840]\n",
            "[Epoch 204/1000] [Batch 101/102] [D loss: -0.365795] [G loss: -1.735535]\n",
            "[Epoch 205/1000] [Batch 101/102] [D loss: -5.485868] [G loss: 13.314374]\n",
            "[Epoch 206/1000] [Batch 101/102] [D loss: -8.233264] [G loss: 7.838936]\n",
            "[Epoch 207/1000] [Batch 101/102] [D loss: -7.188674] [G loss: -6.097935]\n",
            "[Epoch 208/1000] [Batch 101/102] [D loss: -7.154796] [G loss: 4.025554]\n",
            "[Epoch 209/1000] [Batch 101/102] [D loss: -8.163424] [G loss: -3.171828]\n",
            "[Epoch 210/1000] [Batch 101/102] [D loss: -10.041098] [G loss: 6.462368]\n",
            "[Epoch 211/1000] [Batch 101/102] [D loss: -9.512685] [G loss: 1.989874]\n",
            "[Epoch 212/1000] [Batch 101/102] [D loss: -5.837885] [G loss: 0.683876]\n",
            "[Epoch 213/1000] [Batch 101/102] [D loss: -7.962127] [G loss: 10.070220]\n",
            "[Epoch 214/1000] [Batch 101/102] [D loss: -7.958204] [G loss: 3.307265]\n",
            "[Epoch 215/1000] [Batch 101/102] [D loss: -10.321733] [G loss: 3.181343]\n",
            "[Epoch 216/1000] [Batch 101/102] [D loss: -11.128794] [G loss: 4.487166]\n",
            "[Epoch 217/1000] [Batch 101/102] [D loss: -9.776491] [G loss: 3.599970]\n",
            "[Epoch 218/1000] [Batch 101/102] [D loss: -8.008380] [G loss: 0.650829]\n",
            "[Epoch 219/1000] [Batch 101/102] [D loss: -12.212980] [G loss: 1.720784]\n",
            "[Epoch 220/1000] [Batch 101/102] [D loss: -10.286247] [G loss: -4.423490]\n",
            "[Epoch 221/1000] [Batch 101/102] [D loss: -12.617373] [G loss: 2.508542]\n",
            "[Epoch 222/1000] [Batch 101/102] [D loss: -9.110352] [G loss: 3.215004]\n",
            "[Epoch 223/1000] [Batch 101/102] [D loss: -9.088607] [G loss: 7.696617]\n",
            "[Epoch 224/1000] [Batch 101/102] [D loss: -11.692272] [G loss: 8.406561]\n",
            "[Epoch 225/1000] [Batch 101/102] [D loss: -12.094584] [G loss: 15.850411]\n",
            "[Epoch 226/1000] [Batch 101/102] [D loss: -11.287629] [G loss: -6.532219]\n",
            "[Epoch 227/1000] [Batch 101/102] [D loss: -11.584616] [G loss: 3.949558]\n",
            "[Epoch 228/1000] [Batch 101/102] [D loss: -7.603374] [G loss: 9.006084]\n",
            "[Epoch 229/1000] [Batch 101/102] [D loss: -7.714302] [G loss: 6.738967]\n",
            "[Epoch 230/1000] [Batch 101/102] [D loss: -11.028035] [G loss: 2.269385]\n",
            "[Epoch 231/1000] [Batch 101/102] [D loss: -9.675254] [G loss: -4.341802]\n",
            "[Epoch 232/1000] [Batch 101/102] [D loss: -7.889279] [G loss: 10.697487]\n",
            "[Epoch 233/1000] [Batch 101/102] [D loss: -9.608503] [G loss: 5.644187]\n",
            "[Epoch 234/1000] [Batch 101/102] [D loss: -15.641003] [G loss: -1.828750]\n",
            "[Epoch 235/1000] [Batch 101/102] [D loss: -10.967358] [G loss: 13.116001]\n",
            "[Epoch 236/1000] [Batch 101/102] [D loss: -7.912322] [G loss: -2.636222]\n",
            "[Epoch 237/1000] [Batch 101/102] [D loss: -8.571903] [G loss: -6.358265]\n",
            "[Epoch 238/1000] [Batch 101/102] [D loss: -9.467466] [G loss: -6.441086]\n",
            "[Epoch 239/1000] [Batch 101/102] [D loss: -10.487162] [G loss: 3.125635]\n",
            "[Epoch 240/1000] [Batch 101/102] [D loss: -9.938307] [G loss: 8.777223]\n",
            "[Epoch 241/1000] [Batch 101/102] [D loss: -10.055829] [G loss: 4.590399]\n",
            "[Epoch 242/1000] [Batch 101/102] [D loss: -7.761116] [G loss: 8.441277]\n",
            "[Epoch 243/1000] [Batch 101/102] [D loss: -9.630688] [G loss: -1.123410]\n",
            "[Epoch 244/1000] [Batch 101/102] [D loss: -6.763013] [G loss: 10.181108]\n",
            "[Epoch 245/1000] [Batch 101/102] [D loss: -7.013539] [G loss: 5.315413]\n",
            "[Epoch 246/1000] [Batch 101/102] [D loss: -8.302596] [G loss: 5.915664]\n",
            "[Epoch 247/1000] [Batch 101/102] [D loss: -9.461385] [G loss: 11.440829]\n",
            "[Epoch 248/1000] [Batch 101/102] [D loss: -10.362185] [G loss: -1.526663]\n",
            "[Epoch 249/1000] [Batch 101/102] [D loss: -8.539077] [G loss: 2.552174]\n",
            "[Epoch 250/1000] [Batch 101/102] [D loss: -9.811372] [G loss: 0.188733]\n",
            "[Epoch 251/1000] [Batch 101/102] [D loss: -10.224422] [G loss: 12.166138]\n",
            "[Epoch 252/1000] [Batch 101/102] [D loss: -9.878488] [G loss: 11.840274]\n",
            "[Epoch 253/1000] [Batch 101/102] [D loss: -9.849997] [G loss: 5.900465]\n",
            "[Epoch 254/1000] [Batch 101/102] [D loss: -8.950511] [G loss: 5.257705]\n",
            "[Epoch 255/1000] [Batch 101/102] [D loss: -9.015193] [G loss: -5.969341]\n",
            "[Epoch 256/1000] [Batch 101/102] [D loss: -10.936415] [G loss: 6.167114]\n",
            "[Epoch 257/1000] [Batch 101/102] [D loss: -4.941109] [G loss: -4.665672]\n",
            "[Epoch 258/1000] [Batch 101/102] [D loss: -10.487250] [G loss: 2.372811]\n",
            "[Epoch 259/1000] [Batch 101/102] [D loss: -9.763430] [G loss: -0.055603]\n",
            "[Epoch 260/1000] [Batch 101/102] [D loss: -8.625591] [G loss: 6.344629]\n",
            "[Epoch 261/1000] [Batch 101/102] [D loss: -10.399183] [G loss: 3.973079]\n",
            "[Epoch 262/1000] [Batch 101/102] [D loss: -13.405101] [G loss: 2.564637]\n",
            "[Epoch 263/1000] [Batch 101/102] [D loss: -10.702952] [G loss: 21.032364]\n",
            "[Epoch 264/1000] [Batch 101/102] [D loss: -9.490553] [G loss: 6.691244]\n",
            "[Epoch 265/1000] [Batch 101/102] [D loss: -8.382217] [G loss: 6.668887]\n",
            "[Epoch 266/1000] [Batch 101/102] [D loss: -12.457837] [G loss: -8.816872]\n",
            "[Epoch 267/1000] [Batch 101/102] [D loss: -11.688227] [G loss: -7.123584]\n",
            "[Epoch 268/1000] [Batch 101/102] [D loss: -10.132359] [G loss: -1.588844]\n",
            "[Epoch 269/1000] [Batch 101/102] [D loss: -8.430475] [G loss: 21.721233]\n",
            "[Epoch 270/1000] [Batch 101/102] [D loss: -9.226370] [G loss: 1.749936]\n",
            "[Epoch 271/1000] [Batch 101/102] [D loss: -9.833316] [G loss: 4.187392]\n",
            "[Epoch 272/1000] [Batch 101/102] [D loss: -10.519096] [G loss: -2.891216]\n",
            "[Epoch 273/1000] [Batch 101/102] [D loss: -11.920635] [G loss: 7.863153]\n",
            "[Epoch 274/1000] [Batch 101/102] [D loss: -8.784913] [G loss: 4.233679]\n",
            "[Epoch 275/1000] [Batch 101/102] [D loss: -12.325871] [G loss: 3.302076]\n",
            "[Epoch 276/1000] [Batch 101/102] [D loss: -11.045944] [G loss: 8.386677]\n",
            "[Epoch 277/1000] [Batch 101/102] [D loss: -9.822041] [G loss: -0.438142]\n",
            "[Epoch 278/1000] [Batch 101/102] [D loss: -7.895627] [G loss: -1.837091]\n",
            "[Epoch 279/1000] [Batch 101/102] [D loss: -9.580982] [G loss: 7.617812]\n",
            "[Epoch 280/1000] [Batch 101/102] [D loss: -8.967923] [G loss: 2.543835]\n",
            "[Epoch 281/1000] [Batch 101/102] [D loss: -9.970958] [G loss: 16.192017]\n",
            "[Epoch 282/1000] [Batch 101/102] [D loss: -6.567780] [G loss: 10.571333]\n",
            "[Epoch 283/1000] [Batch 101/102] [D loss: -8.795342] [G loss: 9.770807]\n",
            "[Epoch 284/1000] [Batch 101/102] [D loss: -11.734859] [G loss: 5.806040]\n",
            "[Epoch 285/1000] [Batch 101/102] [D loss: -8.276915] [G loss: 9.341597]\n",
            "[Epoch 286/1000] [Batch 101/102] [D loss: -10.306867] [G loss: -2.369068]\n",
            "[Epoch 287/1000] [Batch 101/102] [D loss: -11.730119] [G loss: -2.618280]\n",
            "[Epoch 288/1000] [Batch 101/102] [D loss: -11.470388] [G loss: 5.377494]\n",
            "[Epoch 289/1000] [Batch 101/102] [D loss: -6.060826] [G loss: -0.414256]\n",
            "[Epoch 290/1000] [Batch 101/102] [D loss: -10.240904] [G loss: 11.264793]\n",
            "[Epoch 291/1000] [Batch 101/102] [D loss: -11.045021] [G loss: 9.641844]\n",
            "[Epoch 292/1000] [Batch 101/102] [D loss: -10.426857] [G loss: 3.547175]\n",
            "[Epoch 293/1000] [Batch 101/102] [D loss: -9.979006] [G loss: 17.195662]\n",
            "[Epoch 294/1000] [Batch 101/102] [D loss: -8.268811] [G loss: 9.956664]\n",
            "[Epoch 295/1000] [Batch 101/102] [D loss: -11.205467] [G loss: -3.815149]\n",
            "[Epoch 296/1000] [Batch 101/102] [D loss: -12.428396] [G loss: -5.507209]\n",
            "[Epoch 297/1000] [Batch 101/102] [D loss: -11.845081] [G loss: 3.578162]\n",
            "[Epoch 298/1000] [Batch 101/102] [D loss: -7.794005] [G loss: 5.715205]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7ebeb7d1cd3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Configure input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mreal_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(discriminator, 'drive/MyDrive/d_big_last.pt')\n",
        "torch.save(generator, 'drive/MyDrive/g_big_last.pt')"
      ],
      "metadata": {
        "id": "cM-p6fyh5_rw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "gen_imgs = generator(z)\n",
        "save_image(gen_imgs.data[:32], \"images2/lastN.png\", nrow=8, normalize=True)"
      ],
      "metadata": {
        "id": "EzuW4ei86DSs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "\n",
        "* https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py"
      ],
      "metadata": {
        "id": "s4dv1f-fEc4x"
      }
    }
  ]
}